{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Nasdaq Portfolio ‚Äî Binary Markowitz + Partitioning\n",
        "=================================================\n",
        "\n",
        "* **Step¬†1**  Download 10‚ÄØyears of daily prices for (up to) 48 Nasdaq tickers.\n",
        "* **Step¬†2**  Denoise the covariance matrix via Marchenko‚ÄìPastur (RMT).\n",
        "* **Step¬†3**  Spectral‚Äëcluster the cleaned correlation into N communities (3¬†assets each).\n",
        "* **Step¬†4**  **Global MILP**: binary Markowitz (0/1 weights), select N*3 assets with ‚â•3 per cluster.\n",
        "* **Step¬†5**  **Pipeline**: solve N sub‚ÄëMILPs (3 picks per cluster) and union the results.\n",
        "* **Step¬†6**  Compare objectives ‚Üí approximation ratio.\n",
        "\n",
        "The binary Markowitz objective is the same as in the image:\n",
        "\n",
        "```\n",
        "min   q ¬∑ x·µÄ Œ£ x ¬†‚àí¬† Œº·µÄ x\n",
        "s.t.  x ‚àà {0,1}‚Åø ,  ‚àë x = 12 ,  ‚àë_{i‚ààcluster_j} x_i ‚â• 3\n",
        "```\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "7ddSQ1pMlsdF",
        "outputId": "765d130b-45ea-45b5-fa37-fc79c4a08941"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nNasdaq Portfolio ‚Äî Binary Markowitz + Partitioning\\n=================================================\\n\\n* **Step\\xa01**  Download 10\\u202fyears of daily prices for (up to) 48 Nasdaq tickers.\\n* **Step\\xa02**  Denoise the covariance matrix via Marchenko‚ÄìPastur (RMT).\\n* **Step\\xa03**  Spectral‚Äëcluster the cleaned correlation into *k\\xa0=\\xa04* communities (‚â•3\\xa0assets each).\\n* **Step\\xa04**  **Global MILP**: binary Markowitz (0/1 weights), select **exactly 12** assets with ‚â•3 per cluster.\\n* **Step\\xa05**  **Pipeline**: solve 4 sub‚ÄëMILPs (3 picks per cluster) and union the results.\\n* **Step\\xa06**  Compare objectives ‚Üí approximation ratio.\\n\\nThe binary Markowitz objective is the same as in the image:\\n\\n```\\nmin   q ¬∑ x·µÄ Œ£ x \\xa0‚àí\\xa0 Œº·µÄ x\\ns.t.  x ‚àà {0,1}‚Åø ,  ‚àë x = 12 ,  ‚àë_{i‚ààcluster_j} x_i ‚â• 3\\n```\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib, subprocess, sys, warnings, math, requests, numpy as np, pandas as pd\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Install required packages\n",
        "for pkg in [\"yfinance\",\"numpy\",\"pandas\",\"scipy\",\"scikit-learn\",\"pulp\",\"tqdm\"]:\n",
        "    try:\n",
        "        importlib.import_module(pkg)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",pkg,\"-q\"])\n",
        "\n",
        "import yfinance as yf\n",
        "from numpy.linalg import eigh\n",
        "from sklearn.cluster import SpectralClustering\n",
        "import pulp as pl\n",
        "import random\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "OGwa46nulpLx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "gA2VM1XElqXI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "N_ASSETS           = 150   # number of tickers\n",
        "CLUSTERS           = 10    # spectral clusters\n",
        "ASSETS_PER_CLUSTER = 3     # min picks per cluster\n",
        "PORTFOLIO_SIZE     = CLUSTERS * ASSETS_PER_CLUSTER  # total assets\n",
        "Q                  = 0.5   # risk‚Äëaversion\n",
        "\n",
        "LOOKBACK_YEARS     = 10\n",
        "TRADING_DAYS       = 252\n",
        "MIN_POINTS         = int(TRADING_DAYS * LOOKBACK_YEARS * 0.95)  # ‚â•95% of expected rows\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(f\"NASDAQ PORTFOLIO OPTIMIZATION - {N_ASSETS} Assets\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD86qeT1mZSw",
        "outputId": "0561e8d5-6a9c-457d-9b8b-11a1023e0245"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "NASDAQ PORTFOLIO OPTIMIZATION - 150 Assets\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fetch NASDAQ listings\n",
        "print(\"\\nüìä Fetching NASDAQ listings...\")\n",
        "nasdaq_url = \"https://www.nasdaqtrader.com/dynamic/SymDir/nasdaqlisted.txt\"\n",
        "nasdaq_df = pd.read_csv(nasdaq_url, sep=\"|\", dtype=str)\n",
        "\n",
        "# Clean the data\n",
        "nasdaq_df = nasdaq_df[~nasdaq_df[\"Symbol\"].str.contains(\"File Creation Time\", na=False)]\n",
        "nasdaq_df = nasdaq_df[nasdaq_df[\"Test Issue\"] != \"Y\"]\n",
        "\n",
        "# Filter out non-common stocks\n",
        "bad_suffixes = (\"W\", \"R\", \"P\", \"Q\", \"Y\", \"Z\")  # warrants, rights, preferred, etc.\n",
        "symbols = nasdaq_df[\"Symbol\"].dropna().astype(str).str.strip()\n",
        "tickers = [s for s in symbols if not s.endswith(bad_suffixes) and len(s) <= 5 and s.isalpha()]\n",
        "\n",
        "print(f\"Found {len(tickers)} potential common stocks\")\n",
        "\n",
        "# Shuffle for random selection\n",
        "random.shuffle(tickers)\n",
        "\n",
        "# Step 2: Download price data\n",
        "print(f\"\\nüì• Downloading {LOOKBACK_YEARS}-year price data for {N_ASSETS} assets...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "prices_list, valid_tickers = [], []\n",
        "attempt_count = 0\n",
        "batch_size = 5  # Download in small batches to avoid rate limits\n",
        "\n",
        "pbar = tqdm(total=N_ASSETS, desc=\"Valid assets found\")\n",
        "\n",
        "for i in range(0, len(tickers), batch_size):\n",
        "    if len(valid_tickers) >= N_ASSETS:\n",
        "        break\n",
        "\n",
        "    batch = tickers[i:i+batch_size]\n",
        "    attempt_count += len(batch)\n",
        "\n",
        "    try:\n",
        "        # Download batch\n",
        "        data = yf.download(\n",
        "            batch,\n",
        "            period=f\"{LOOKBACK_YEARS}y\",\n",
        "            interval=\"1d\",\n",
        "            auto_adjust=True,\n",
        "            progress=False,\n",
        "            threads=False\n",
        "        )\n",
        "\n",
        "        # Handle single ticker case\n",
        "        if len(batch) == 1 and not data.empty:\n",
        "            close_data = data[\"Close\"] if \"Close\" in data.columns else data\n",
        "            if not isinstance(close_data, pd.DataFrame):\n",
        "                close_data = pd.DataFrame({batch[0]: close_data})\n",
        "        else:\n",
        "            close_data = data[\"Close\"] if \"Close\" in data.columns else data\n",
        "\n",
        "        # Check each ticker in batch\n",
        "        for ticker in batch:\n",
        "            if len(valid_tickers) >= N_ASSETS:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                if ticker in close_data.columns:\n",
        "                    ticker_data = close_data[ticker].dropna()\n",
        "\n",
        "                    # Check if we have enough data points\n",
        "                    if len(ticker_data) >= MIN_POINTS:\n",
        "                        ticker_data.name = ticker\n",
        "                        prices_list.append(ticker_data)\n",
        "                        valid_tickers.append(ticker)\n",
        "                        pbar.update(1)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    except Exception as e:\n",
        "        # Rate limit handling\n",
        "        if \"429\" in str(e) or \"rate\" in str(e).lower():\n",
        "            time.sleep(2)\n",
        "        continue\n",
        "\n",
        "    # Small delay between batches\n",
        "    time.sleep(0.1)\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "if len(valid_tickers) < N_ASSETS:\n",
        "    print(f\"\\n‚ö†Ô∏è  Only found {len(valid_tickers)} assets with {LOOKBACK_YEARS}-year history\")\n",
        "    print(\"Continuing with available assets...\")\n",
        "    N_ASSETS = len(valid_tickers)\n",
        "    PORTFOLIO_SIZE = min(PORTFOLIO_SIZE, N_ASSETS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PecTeSq-meDq",
        "outputId": "dbb2dd72-834c-4ffc-88c9-abdd9665e3e4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Fetching NASDAQ listings...\n",
            "Found 3818 potential common stocks\n",
            "\n",
            "üì• Downloading 10-year price data for 150 assets...\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Valid assets found: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 150/150 [01:05<00:00,  2.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Build price matrix and calculate returns\n",
        "print(f\"\\nüìà Building price matrix for {len(valid_tickers)} assets...\")\n",
        "prices = pd.concat(prices_list, axis=1)\n",
        "returns = prices.pct_change().dropna()\n",
        "\n",
        "# Remove any remaining NaN columns\n",
        "valid_cols = returns.columns[returns.notna().all()]\n",
        "returns = returns[valid_cols]\n",
        "prices = prices[valid_cols]\n",
        "valid_tickers = list(valid_cols)\n",
        "\n",
        "print(f\"Final dataset: {len(valid_tickers)} assets √ó {len(returns)} days\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo8FhEobmnbY",
        "outputId": "ff57635f-c2c0-4618-bfa3-da949e874529"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìà Building price matrix for 150 assets...\n",
            "Final dataset: 150 assets √ó 2396 days\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Calculate statistics\n",
        "print(\"\\nüßÆ Calculating return and risk metrics...\")\n",
        "mu = returns.mean().values * TRADING_DAYS  # Annualized returns\n",
        "sigma = returns.cov().values * TRADING_DAYS  # Annualized covariance\n",
        "\n",
        "# Save metrics for reference\n",
        "metrics_df = pd.DataFrame({\n",
        "    \"Asset\": valid_tickers,\n",
        "    \"Annual_Return\": mu,\n",
        "    \"Annual_Volatility\": np.sqrt(np.diag(sigma)),\n",
        "    \"Sharpe_Ratio\": mu / np.sqrt(np.diag(sigma))\n",
        "}).round(6)\n",
        "\n",
        "metrics_df.to_csv(\"portfolio_metrics.csv\", index=False)\n",
        "print(\"Saved metrics to portfolio_metrics.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_-wYuf_mqaj",
        "outputId": "14b4d802-fccd-496d-c746-7538dadca0db"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üßÆ Calculating return and risk metrics...\n",
            "Saved metrics to portfolio_metrics.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: RMT Denoising\n",
        "def rmt_denoise(sigma, n_obs):\n",
        "    \"\"\"RMT denoising with stability improvements.\"\"\"\n",
        "    n = len(sigma)\n",
        "\n",
        "    # Ensure symmetric\n",
        "    sigma = (sigma + sigma.T) / 2\n",
        "\n",
        "    # Add regularization\n",
        "    sigma += np.eye(n) * 1e-8\n",
        "\n",
        "    # Calculate correlation matrix\n",
        "    std = np.sqrt(np.diag(sigma))\n",
        "    std = np.maximum(std, 1e-10)\n",
        "    corr = sigma / np.outer(std, std)\n",
        "    np.fill_diagonal(corr, 1.0)\n",
        "\n",
        "    # Eigenvalue decomposition\n",
        "    vals, vecs = eigh(corr)\n",
        "    vals, vecs = vals[::-1], vecs[:, ::-1]\n",
        "\n",
        "    # Marchenko-Pastur bounds\n",
        "    q = n / n_obs\n",
        "    if q >= 1:\n",
        "        q = 0.99\n",
        "\n",
        "    lmin = (1 - np.sqrt(q)) ** 2\n",
        "    lmax = (1 + np.sqrt(q)) ** 2\n",
        "\n",
        "    # Clean eigenvalues\n",
        "    clean_vals = vals.copy()\n",
        "    noise_idx = (vals >= lmin) & (vals <= lmax)\n",
        "\n",
        "    if np.any(noise_idx):\n",
        "        avg_noise = np.mean(vals[noise_idx])\n",
        "        clean_vals[noise_idx] = avg_noise\n",
        "\n",
        "    # Remove market mode if dominant\n",
        "    if clean_vals[0] > lmax * 2:\n",
        "        clean_vals[0] = clean_vals[1]\n",
        "\n",
        "    # Ensure positive\n",
        "    clean_vals = np.maximum(clean_vals, 1e-8)\n",
        "\n",
        "    # Reconstruct\n",
        "    corr_clean = (vecs * clean_vals) @ vecs.T\n",
        "    corr_clean = (corr_clean + corr_clean.T) / 2\n",
        "    np.fill_diagonal(corr_clean, 1.0)\n",
        "\n",
        "    return corr_clean * np.outer(std, std)\n",
        "\n",
        "print(\"\\nüîß Applying RMT denoising...\")\n",
        "sigma_clean = rmt_denoise(sigma, len(returns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmhWKYh_muOq",
        "outputId": "afca6057-ec11-471c-9b53-a3050785d2e8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîß Applying RMT denoising...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Spectral Clustering\n",
        "def spectral_clusters(corr, k, min_sz):\n",
        "    \"\"\"Spectral clustering on correlation matrix.\"\"\"\n",
        "    n = len(corr)\n",
        "\n",
        "    # Affinity matrix\n",
        "    aff = np.abs(corr) ** 2\n",
        "    np.fill_diagonal(aff, 1.0)\n",
        "    aff = (aff + aff.T) / 2\n",
        "\n",
        "    # Clustering\n",
        "    try:\n",
        "        clustering = SpectralClustering(\n",
        "            n_clusters=k,\n",
        "            affinity='precomputed',\n",
        "            random_state=42,\n",
        "            n_init=10\n",
        "        )\n",
        "        labels = clustering.fit_predict(aff)\n",
        "    except:\n",
        "        # Fallback to random assignment\n",
        "        labels = np.random.randint(0, k, size=n)\n",
        "\n",
        "    clusters = [np.where(labels == i)[0].tolist() for i in range(k)]\n",
        "    clusters = [c for c in clusters if len(c) > 0]\n",
        "\n",
        "    # Merge small clusters\n",
        "    i = 0\n",
        "    while i < len(clusters):\n",
        "        if len(clusters[i]) < min_sz and len(clusters) > 1:\n",
        "            small = clusters.pop(i)\n",
        "            # Add to random cluster\n",
        "            if clusters:\n",
        "                clusters[np.random.randint(len(clusters))].extend(small)\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    # Ensure exactly k clusters\n",
        "    while len(clusters) < k and any(len(c) >= 2*min_sz for c in clusters):\n",
        "        # Split largest\n",
        "        largest_idx = max(range(len(clusters)), key=lambda i: len(clusters[i]))\n",
        "        large = clusters[largest_idx]\n",
        "        mid = len(large) // 2\n",
        "        clusters[largest_idx] = large[:mid]\n",
        "        clusters.append(large[mid:])\n",
        "\n",
        "    return clusters[:k]\n",
        "\n",
        "# Calculate correlation for clustering\n",
        "std = np.sqrt(np.diag(sigma_clean))\n",
        "std = np.maximum(std, 1e-10)\n",
        "corr_clean = sigma_clean / np.outer(std, std)\n",
        "\n",
        "print(f\"\\nüéØ Clustering into {CLUSTERS} groups...\")\n",
        "clusters = spectral_clusters(corr_clean, CLUSTERS, ASSETS_PER_CLUSTER)\n",
        "print(\"Cluster sizes:\", [len(c) for c in clusters])\n",
        "\n",
        "# Create membership array\n",
        "membership = np.zeros(len(valid_tickers), dtype=int)\n",
        "for ci, cluster in enumerate(clusters):\n",
        "    for idx in cluster:\n",
        "        if idx < len(valid_tickers):\n",
        "            membership[idx] = ci"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N50O29IAmxaH",
        "outputId": "726a2456-2cad-42ab-e88b-47475ff75729"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üéØ Clustering into 10 groups...\n",
            "Cluster sizes: [5, 20, 19, 15, 24, 7, 16, 14, 19, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Binary Optimization\n",
        "def optimise_binary(mu, sigma, K, q, membership, min_per, time_limit=3000):\n",
        "    \"\"\"\n",
        "    Binary Markowitz optimization with cluster constraints and solver diagnostics.\n",
        "\n",
        "    Args:\n",
        "        mu (array): Expected returns (length n).\n",
        "        sigma (2D array): Covariance matrix (n x n).\n",
        "        K (int): Number of assets to select.\n",
        "        q (float): Risk aversion parameter.\n",
        "        membership (array): Cluster assignment for each asset (length n).\n",
        "        min_per (int): Minimum assets per cluster.\n",
        "        time_limit (int): Solver time limit in seconds (default 300).\n",
        "\n",
        "    Returns:\n",
        "        selected (list): Indices of selected assets.\n",
        "        obj_value (float): Objective value.\n",
        "    \"\"\"\n",
        "    n = len(mu)\n",
        "    assert 1 <= K <= n, f\"K must be between 1 and {n} (got {K})\"\n",
        "\n",
        "    # Variables\n",
        "    x = pl.LpVariable.dicts(\"x\", range(n), 0, 1, pl.LpBinary)\n",
        "    y = {(i, j): pl.LpVariable(f\"y_{i}_{j}\", 0, 1, pl.LpBinary)\n",
        "         for i in range(n) for j in range(i, n)}\n",
        "\n",
        "    # Problem\n",
        "    prob = pl.LpProblem(\"BinMarkowitz\", pl.LpMinimize)\n",
        "\n",
        "    # Linearization constraints\n",
        "    for (i, j), yij in y.items():\n",
        "        prob += yij <= x[i]\n",
        "        prob += yij <= x[j]\n",
        "        prob += yij >= x[i] + x[j] - 1\n",
        "\n",
        "    # Objective\n",
        "    prob += (pl.lpSum(q * sigma[i, j] * y[(i, j)] * (2 if i != j else 1)\n",
        "                      for (i, j) in y) -\n",
        "             pl.lpSum(mu[i] * x[i] for i in range(n)))\n",
        "\n",
        "    # Cardinality constraint\n",
        "    prob += pl.lpSum(x.values()) == K\n",
        "\n",
        "    # Cluster constraints\n",
        "    for c in set(membership):\n",
        "        idx = [i for i in range(n) if membership[i] == c]\n",
        "        if len(idx) >= min_per:\n",
        "            prob += pl.lpSum(x[i] for i in idx) >= min_per\n",
        "\n",
        "    # Solve with time limit and print solver status\n",
        "    status = prob.solve(pl.PULP_CBC_CMD(msg=True, timeLimit=time_limit))\n",
        "    status_str = pl.LpStatus[prob.status]\n",
        "    print(f\"Solver status: {status_str}\")\n",
        "\n",
        "    if status_str != \"Optimal\":\n",
        "        print(\"Warning: Solver did not find an optimal solution. Check constraints and cluster sizes.\")\n",
        "\n",
        "    selected = [i for i in range(n) if x[i] is not None and pl.value(x[i]) > 0.5]\n",
        "    obj_value = pl.value(prob.objective) if prob.objective is not None else 0\n",
        "\n",
        "    return selected, obj_value"
      ],
      "metadata": {
        "id": "6lsOMnVEm0vR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Global Optimization\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üåê GLOBAL OPTIMIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    global_idx, global_obj = optimise_binary(\n",
        "        mu, sigma_clean, PORTFOLIO_SIZE, Q, membership, ASSETS_PER_CLUSTER\n",
        "    )\n",
        "    print(f\"Objective value: {global_obj:.6f}\")\n",
        "    print(f\"Selected {len(global_idx)} assets\")\n",
        "except Exception as e:\n",
        "    print(f\"Optimization failed: {e}\")\n",
        "    # Fallback: select top Sharpe ratio assets\n",
        "    sharpe = mu / np.sqrt(np.diag(sigma_clean))\n",
        "    global_idx = np.argsort(sharpe)[-PORTFOLIO_SIZE:].tolist()\n",
        "    global_obj = 0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNuDcXvrm3Nu",
        "outputId": "2e852793-b0b6-435a-dd99-92c2810d8c04"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üåê GLOBAL OPTIMIZATION\n",
            "============================================================\n",
            "Solver status: Optimal\n",
            "Objective value: -2.724654\n",
            "Selected 30 assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzmpIwrvhh3L",
        "outputId": "de69ecab-8dc2-4988-e47e-20acde0d7d04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üîó PIPELINE OPTIMIZATION\n",
            "============================================================\n",
            "Solver status: Optimal\n",
            "Cluster 0: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 1: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 2: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 3: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 4: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 5: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 6: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 7: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 8: selected 3 assets\n",
            "Solver status: Optimal\n",
            "Cluster 9: selected 3 assets\n"
          ]
        }
      ],
      "source": [
        "# Step 9: Pipeline Optimization\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üîó PIPELINE OPTIMIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pipe_idx = []\n",
        "for ci, cluster in enumerate(clusters):\n",
        "    if len(cluster) < ASSETS_PER_CLUSTER:\n",
        "        continue\n",
        "\n",
        "    sub_mu = mu[cluster]\n",
        "    sub_sig = sigma_clean[np.ix_(cluster, cluster)]\n",
        "\n",
        "    try:\n",
        "        picks, _ = optimise_binary(\n",
        "            sub_mu, sub_sig,\n",
        "            min(ASSETS_PER_CLUSTER, len(cluster)),\n",
        "            Q * 0.8,  # Slightly lower risk aversion\n",
        "            [0] * len(cluster),\n",
        "            min(ASSETS_PER_CLUSTER, len(cluster))\n",
        "        )\n",
        "        pipe_idx.extend([cluster[i] for i in picks])\n",
        "        print(f\"Cluster {ci}: selected {len(picks)} assets\")\n",
        "    except:\n",
        "        # Fallback to top Sharpe in cluster\n",
        "        sharpe = sub_mu / np.sqrt(np.diag(sub_sig))\n",
        "        top_k = min(ASSETS_PER_CLUSTER, len(cluster))\n",
        "        best = np.argsort(sharpe)[-top_k:]\n",
        "        pipe_idx.extend([cluster[i] for i in best])\n",
        "\n",
        "# Ensure correct portfolio size\n",
        "pipe_idx = pipe_idx[:PORTFOLIO_SIZE]\n",
        "\n",
        "# Calculate pipeline objective\n",
        "pipe_x = np.zeros(len(mu))\n",
        "for idx in pipe_idx:\n",
        "    if idx < len(pipe_x):\n",
        "        pipe_x[idx] = 1\n",
        "pipe_obj = Q * pipe_x @ sigma_clean @ pipe_x - mu @ pipe_x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Global objective:   {global_obj:.6f}\")\n",
        "print(f\"Pipeline objective: {pipe_obj:.6f}\")\n",
        "if global_obj != 0:\n",
        "    print(f\"Approximation ratio: {pipe_obj/global_obj:.6f}\")\n",
        "\n",
        "print(f\"\\nüèÜ GLOBAL PORTFOLIO ({len(global_idx)} assets):\")\n",
        "global_tickers = [valid_tickers[i] for i in global_idx if i < len(valid_tickers)]\n",
        "print(\", \".join(global_tickers[:10]) + (\"...\" if len(global_tickers) > 10 else \"\"))\n",
        "\n",
        "print(f\"\\nüîó PIPELINE PORTFOLIO ({len(pipe_idx)} assets):\")\n",
        "pipe_tickers = [valid_tickers[i] for i in pipe_idx if i < len(valid_tickers)]\n",
        "print(\", \".join(pipe_tickers[:10]) + (\"...\" if len(pipe_tickers) > 10 else \"\"))\n",
        "\n",
        "# Save results\n",
        "results_df = pd.DataFrame({\n",
        "    \"Method\": [\"Global\", \"Pipeline\"],\n",
        "    \"Objective\": [global_obj, pipe_obj],\n",
        "    \"Assets\": [len(global_idx), len(pipe_idx)]\n",
        "})\n",
        "results_df.to_csv(\"optimization_results.csv\", index=False)\n",
        "\n",
        "# Save portfolios\n",
        "pd.DataFrame({\"Global_Portfolio\": global_tickers}).to_csv(\"global_portfolio.csv\", index=False)\n",
        "pd.DataFrame({\"Pipeline_Portfolio\": pipe_tickers}).to_csv(\"pipeline_portfolio.csv\", index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Results saved to:\")\n",
        "print(\"   - portfolio_metrics.csv\")\n",
        "print(\"   - optimization_results.csv\")\n",
        "print(\"   - global_portfolio.csv\")\n",
        "print(\"   - pipeline_portfolio.csv\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8f3EXipm7_N",
        "outputId": "7f3a4dd8-6d31-4ed7-dc09-a8022ef60aa4"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "üìä RESULTS SUMMARY\n",
            "============================================================\n",
            "Global objective:   -2.724654\n",
            "Pipeline objective: -2.750189\n",
            "Approximation ratio: 1.009372\n",
            "\n",
            "üèÜ GLOBAL PORTFOLIO (30 assets):\n",
            "LFMD, BIS, PUI, MMSI, DXPE, BOKF, VGSH, CHCO, RING, BPOPM...\n",
            "\n",
            "üîó PIPELINE PORTFOLIO (30 assets):\n",
            "PUI, CDL, LVHD, CHCO, TSBK, TFIN, WILC, LINC, CLMB, LFMD...\n",
            "\n",
            "‚úÖ Results saved to:\n",
            "   - portfolio_metrics.csv\n",
            "   - optimization_results.csv\n",
            "   - global_portfolio.csv\n",
            "   - pipeline_portfolio.csv\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}