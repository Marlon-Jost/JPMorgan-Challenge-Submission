{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391238dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "from numpy.linalg import eigh\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from itertools import combinations\n",
    "\n",
    "# QOKit imports (keep these unchanged)\n",
    "from qokit.portfolio_optimization import get_problem, portfolio_brute_force, get_sk_ini\n",
    "from qokit.qaoa_circuit_portfolio import (\n",
    "    get_qaoa_circuit,\n",
    "    get_parameterized_qaoa_circuit,\n",
    "    get_energy_expectation_sv,\n",
    ")\n",
    "from qokit.qaoa_objective_portfolio import get_qaoa_portfolio_objective\n",
    "from qokit.utils import reverse_array_index_bit_order\n",
    "from qiskit.quantum_info import Statevector\n",
    "from qiskit import transpile\n",
    "from qiskit_aer import Aer\n",
    "import nlopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefa709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Advanced decomposition functions from the research paper\n",
    "def rmt_denoise(sigma, n_obs):\n",
    "    \"\"\"RMT denoising with stability improvements.\"\"\"\n",
    "    n = len(sigma)\n",
    "    \n",
    "    # Ensure symmetric\n",
    "    sigma = (sigma + sigma.T) / 2\n",
    "    \n",
    "    # Add regularization\n",
    "    sigma += np.eye(n) * 1e-8\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    std = np.sqrt(np.diag(sigma))\n",
    "    std = np.maximum(std, 1e-10)\n",
    "    corr = sigma / np.outer(std, std)\n",
    "    np.fill_diagonal(corr, 1.0)\n",
    "    \n",
    "    # Eigenvalue decomposition\n",
    "    vals, vecs = eigh(corr)\n",
    "    vals, vecs = vals[::-1], vecs[:, ::-1]\n",
    "    \n",
    "    # Marchenko-Pastur bounds\n",
    "    q = n / n_obs\n",
    "    if q >= 1:\n",
    "        q = 0.99\n",
    "    \n",
    "    lmin = (1 - np.sqrt(q)) ** 2\n",
    "    lmax = (1 + np.sqrt(q)) ** 2\n",
    "    \n",
    "    # Clean eigenvalues\n",
    "    clean_vals = vals.copy()\n",
    "    noise_idx = (vals >= lmin) & (vals <= lmax)\n",
    "    \n",
    "    if np.any(noise_idx):\n",
    "        avg_noise = np.mean(vals[noise_idx])\n",
    "        clean_vals[noise_idx] = avg_noise\n",
    "    \n",
    "    # Remove market mode if dominant\n",
    "    if clean_vals[0] > lmax * 2:\n",
    "        clean_vals[0] = clean_vals[1]\n",
    "    \n",
    "    # Ensure positive\n",
    "    clean_vals = np.maximum(clean_vals, 1e-8)\n",
    "    \n",
    "    # Reconstruct\n",
    "    corr_clean = (vecs * clean_vals) @ vecs.T\n",
    "    corr_clean = (corr_clean + corr_clean.T) / 2\n",
    "    np.fill_diagonal(corr_clean, 1.0)\n",
    "    \n",
    "    return corr_clean * np.outer(std, std)\n",
    "\n",
    "\n",
    "def spectral_clusters(corr, k, min_sz):\n",
    "    \"\"\"Spectral clustering on correlation matrix.\"\"\"\n",
    "    n = len(corr)\n",
    "    \n",
    "    # Affinity matrix\n",
    "    aff = np.abs(corr) ** 2\n",
    "    np.fill_diagonal(aff, 1.0)\n",
    "    aff = (aff + aff.T) / 2\n",
    "    \n",
    "    # Clustering\n",
    "    try:\n",
    "        clustering = SpectralClustering(\n",
    "            n_clusters=k,\n",
    "            affinity='precomputed',\n",
    "            random_state=42,\n",
    "            n_init=10\n",
    "        )\n",
    "        labels = clustering.fit_predict(aff)\n",
    "    except:\n",
    "        # Fallback to random assignment\n",
    "        labels = np.random.randint(0, k, size=n)\n",
    "    \n",
    "    clusters = [np.where(labels == i)[0].tolist() for i in range(k)]\n",
    "    clusters = [c for c in clusters if len(c) > 0]\n",
    "    \n",
    "    # Merge small clusters\n",
    "    i = 0\n",
    "    while i < len(clusters):\n",
    "        if len(clusters[i]) < min_sz and len(clusters) > 1:\n",
    "            small = clusters.pop(i)\n",
    "            # Add to random cluster\n",
    "            if clusters:\n",
    "                clusters[np.random.randint(len(clusters))].extend(small)\n",
    "        else:\n",
    "            i += 1\n",
    "    \n",
    "    # Ensure exactly k clusters if possible\n",
    "    while len(clusters) < k and any(len(c) >= 2*min_sz for c in clusters):\n",
    "        # Split largest\n",
    "        largest_idx = max(range(len(clusters)), key=lambda i: len(clusters[i]))\n",
    "        large = clusters[largest_idx]\n",
    "        mid = len(large) // 2\n",
    "        clusters[largest_idx] = large[:mid]\n",
    "        clusters.append(large[mid:])\n",
    "    \n",
    "    return clusters[:k]\n",
    "\n",
    "\n",
    "def partition_with_rmt(sigma, mu, size, n_obs=None):\n",
    "    \"\"\"\n",
    "    Advanced partitioning using RMT denoising and spectral clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sigma : np.ndarray\n",
    "        Covariance matrix\n",
    "    mu : np.ndarray\n",
    "        Expected returns vector\n",
    "    size : int\n",
    "        Target size for each partition\n",
    "    n_obs : int\n",
    "        Number of observations used to compute covariance (for RMT)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    partitioned_mus : List of expected return sub-vectors\n",
    "    partitioned_sigmas : List of covariance sub-matrices\n",
    "    mappings : List of indices for each partition\n",
    "    \"\"\"\n",
    "    n = len(mu)\n",
    "    \n",
    "    # Step 1: Apply RMT denoising\n",
    "    if n_obs is None:\n",
    "        n_obs = 252 * 5  # Default to 5 years of daily data\n",
    "    \n",
    "    sigma_clean = rmt_denoise(sigma, n_obs)\n",
    "    \n",
    "    # Step 2: Calculate correlation for clustering\n",
    "    std = np.sqrt(np.diag(sigma_clean))\n",
    "    std = np.maximum(std, 1e-10)\n",
    "    corr_clean = sigma_clean / np.outer(std, std)\n",
    "    \n",
    "    # Step 3: Determine number of clusters\n",
    "    n_clusters = max(1, n // size)\n",
    "    \n",
    "    # Step 4: Perform spectral clustering\n",
    "    clusters = spectral_clusters(corr_clean, n_clusters, min_sz=3)\n",
    "    \n",
    "    # Step 5: Extract submatrices and sub-vectors\n",
    "    partitioned_sigmas = []\n",
    "    partitioned_mus = []\n",
    "    mappings = []\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        if len(cluster) > 0:\n",
    "            cluster_array = np.array(cluster)\n",
    "            sub_sigma = sigma_clean[np.ix_(cluster_array, cluster_array)]\n",
    "            sub_mu = mu[cluster_array]\n",
    "            \n",
    "            partitioned_sigmas.append(sub_sigma)\n",
    "            partitioned_mus.append(sub_mu)\n",
    "            mappings.append(cluster)\n",
    "    \n",
    "    return partitioned_mus, partitioned_sigmas, mappings\n",
    "\n",
    "\n",
    "def get_sub_problem_with_rebalancing(sigma, mu, K, q, seed=1, scale=1.0, \n",
    "                                    full_sigma=None, full_mu=None):\n",
    "    \"\"\"\n",
    "    Create subproblem with risk rebalancing as in the paper.\n",
    "    \n",
    "    Includes the risk rebalancing formula from Eq. (15) in the paper.\n",
    "    \"\"\"\n",
    "    po_problem = {\n",
    "        \"N\": len(mu),\n",
    "        \"K\": K,\n",
    "        \"q\": q,\n",
    "        \"seed\": seed,\n",
    "        \"means\": mu,\n",
    "        \"cov\": sigma,\n",
    "        \"pre\": False\n",
    "    }\n",
    "    \n",
    "    # Apply risk rebalancing if full problem info is provided\n",
    "    if full_sigma is not None and full_mu is not None:\n",
    "        # Risk rebalancing formula from the paper\n",
    "        mu_ratio = np.linalg.norm(mu) / np.linalg.norm(full_mu)\n",
    "        sigma_ratio = np.linalg.norm(sigma, 'fro') / np.linalg.norm(full_sigma, 'fro')\n",
    "        q_rebalanced = q * (mu_ratio / sigma_ratio)\n",
    "        po_problem[\"q\"] = q_rebalanced\n",
    "    \n",
    "    # Apply scaling\n",
    "    po_problem[\"scale\"] = scale\n",
    "    po_problem[\"means\"] = scale * np.array(mu)\n",
    "    po_problem[\"cov\"] = scale * np.array(sigma)\n",
    "    \n",
    "    return po_problem\n",
    "\n",
    "\n",
    "def map_qaoa_outputs_to_full_vector(outputs, mappings, n):\n",
    "    \"\"\"Map QAOA outputs back to full solution vector.\"\"\"\n",
    "    result = np.zeros(n, dtype=int)\n",
    "    \n",
    "    for bits, indices in zip(outputs, mappings):\n",
    "        for bit, idx in zip(bits, indices):\n",
    "            result[idx] = int(bit)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def minimize_nlopt(f, x0, p):\n",
    "    \"\"\"Minimize using NLopt BOBYQA algorithm.\"\"\"\n",
    "    def nlopt_wrapper(x, grad):\n",
    "        return f(x).real\n",
    "    \n",
    "    opt = nlopt.opt(nlopt.LN_BOBYQA, 2 * p)\n",
    "    opt.set_min_objective(nlopt_wrapper)\n",
    "    opt.set_xtol_rel(1e-8)\n",
    "    opt.set_ftol_rel(1e-8)\n",
    "    opt.set_initial_step(0.01)\n",
    "    \n",
    "    xstar = opt.optimize(x0)\n",
    "    minf = opt.last_optimum_value()\n",
    "    \n",
    "    return xstar, minf\n",
    "\n",
    "\n",
    "def solve_qaoa_subproblem(po_problem, p=5, verbose=False):\n",
    "    \"\"\"\n",
    "    Solve a single portfolio optimization subproblem using QAOA.\n",
    "    \n",
    "    Returns:\n",
    "    - best_bitstring: The optimal asset selection as a bitstring\n",
    "    - opt_params: The optimized QAOA parameters\n",
    "    - ar: The approximation ratio for this subproblem\n",
    "    \"\"\"\n",
    "    N = po_problem['N']\n",
    "    K = po_problem['K']\n",
    "    \n",
    "    # Create QAOA objective function\n",
    "    qaoa_obj = get_qaoa_portfolio_objective(\n",
    "        po_problem=po_problem,\n",
    "        p=p,\n",
    "        ini='dicke',  # Dicke state initialization\n",
    "        mixer='trotter_ring',  # Ring-XY mixer\n",
    "        T=1,\n",
    "        simulator='python'  # QOKit simulator\n",
    "    )\n",
    "    \n",
    "    # Get initial parameters\n",
    "    x0 = get_sk_ini(p=p)\n",
    "    \n",
    "    # Optimize QAOA parameters\n",
    "    if verbose:\n",
    "        print(f\"  Optimizing {N}-asset subproblem with K={K}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    opt_params, opt_energy = minimize_nlopt(qaoa_obj, x0, p=p)\n",
    "    opt_time = time.time() - start_time\n",
    "    \n",
    "    # Get the quantum state with optimal parameters\n",
    "    gammas = opt_params[:p] / 2\n",
    "    betas = opt_params[p:] / 2\n",
    "    \n",
    "    # Create circuit and get statevector\n",
    "    qc = get_qaoa_circuit(po_problem, gammas=gammas, betas=betas, depth=p)\n",
    "    sv = Statevector.from_instruction(qc)\n",
    "    probs = sv.probabilities()\n",
    "    \n",
    "    # Find the most probable valid state (with K assets)\n",
    "    best_prob = 0\n",
    "    best_bitstring = None\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        bitstring = format(i, f'0{N}b')\n",
    "        if bitstring.count('1') == K and prob > best_prob:\n",
    "            best_prob = prob\n",
    "            best_bitstring = bitstring\n",
    "    \n",
    "    # Calculate approximation ratio\n",
    "    best_classical = portfolio_brute_force(po_problem, return_bitstring=False)\n",
    "    ar = (opt_energy - best_classical[1]) / (best_classical[0] - best_classical[1])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"    Optimization time: {opt_time:.2f}s, AR: {ar:.3f}, Best prob: {best_prob:.3f}\")\n",
    "    \n",
    "    return best_bitstring, opt_params, ar\n",
    "\n",
    "\n",
    "def load_portfolio_data(pickle_file: str):\n",
    "    \"\"\"Load portfolio data from NetworkX graph pickle file.\"\"\"\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    if hasattr(data, 'nodes') and hasattr(data, 'edges'):\n",
    "        graph = data\n",
    "        nodes = list(graph.nodes())\n",
    "        n = len(nodes)\n",
    "        \n",
    "        # Extract returns and volatilities\n",
    "        mu = np.zeros(n)\n",
    "        volatilities = np.zeros(n)\n",
    "        \n",
    "        for i, node in enumerate(nodes):\n",
    "            node_data = graph.nodes[node]\n",
    "            mu[i] = node_data.get('annual_return', 0)\n",
    "            volatilities[i] = node_data.get('annual_volatility', 0)\n",
    "        \n",
    "        # Build correlation matrix from edge weights\n",
    "        correlation = np.eye(n)\n",
    "        node_to_idx = {node: i for i, node in enumerate(nodes)}\n",
    "        \n",
    "        for node1, node2, edge_data in graph.edges(data=True):\n",
    "            i = node_to_idx[node1]\n",
    "            j = node_to_idx[node2]\n",
    "            weight = edge_data.get('weight', 0)\n",
    "            correlation[i, j] = weight\n",
    "            correlation[j, i] = weight\n",
    "        \n",
    "        # Convert correlation to covariance\n",
    "        sigma = np.outer(volatilities, volatilities) * correlation\n",
    "        \n",
    "        return sigma, mu\n",
    "    \n",
    "    raise ValueError(\"Expected NetworkX graph in pickle file\")\n",
    "\n",
    "\n",
    "def calculate_final_approximation_ratio(x: np.ndarray, sigma: np.ndarray, mu: np.ndarray, \n",
    "                                      K: int, q: float, n_samples: int = 1000) -> float:\n",
    "    \"\"\"\n",
    "    Calculate approximation ratio for the final solution.\n",
    "    \n",
    "    For large problems where brute force is infeasible, we sample random solutions\n",
    "    to estimate the worst case and use heuristics for the best case.\n",
    "    \"\"\"\n",
    "    n = len(mu)\n",
    "    current_obj = q * x.T @ sigma @ x - mu.T @ x\n",
    "    \n",
    "    # For small problems, use brute force\n",
    "    if n <= 20:\n",
    "        from itertools import combinations\n",
    "        \n",
    "        best_obj = float('inf')\n",
    "        worst_obj = float('-inf')\n",
    "        \n",
    "        for selected in combinations(range(n), K):\n",
    "            x_temp = np.zeros(n)\n",
    "            x_temp[list(selected)] = 1\n",
    "            obj = q * x_temp.T @ sigma @ x_temp - mu.T @ x_temp\n",
    "            \n",
    "            best_obj = min(best_obj, obj)\n",
    "            worst_obj = max(worst_obj, obj)\n",
    "    else:\n",
    "        # For larger problems, use sampling and heuristics\n",
    "        \n",
    "        # Estimate worst case by sampling\n",
    "        worst_obj = float('-inf')\n",
    "        for _ in range(n_samples):\n",
    "            indices = np.random.choice(n, K, replace=False)\n",
    "            x_temp = np.zeros(n)\n",
    "            x_temp[indices] = 1\n",
    "            obj = q * x_temp.T @ sigma @ x_temp - mu.T @ x_temp\n",
    "            worst_obj = max(worst_obj, obj)\n",
    "        \n",
    "        # Estimate best case using greedy heuristic\n",
    "        # Start with assets that have best return/risk ratio\n",
    "        scores = mu - q * np.diag(sigma)\n",
    "        best_indices = np.argsort(scores)[-K:]\n",
    "        x_best = np.zeros(n)\n",
    "        x_best[best_indices] = 1\n",
    "        best_obj = q * x_best.T @ sigma @ x_best - mu.T @ x_best\n",
    "        \n",
    "        # Refine with local search\n",
    "        for _ in range(100):\n",
    "            improved = False\n",
    "            for i in range(n):\n",
    "                if x_best[i] == 1:\n",
    "                    for j in range(n):\n",
    "                        if x_best[j] == 0:\n",
    "                            # Try swapping\n",
    "                            x_best[i] = 0\n",
    "                            x_best[j] = 1\n",
    "                            new_obj = q * x_best.T @ sigma @ x_best - mu.T @ x_best\n",
    "                            if new_obj < best_obj:\n",
    "                                best_obj = new_obj\n",
    "                                improved = True\n",
    "                                break\n",
    "                            else:\n",
    "                                # Revert\n",
    "                                x_best[i] = 1\n",
    "                                x_best[j] = 0\n",
    "                    if improved:\n",
    "                        break\n",
    "            if not improved:\n",
    "                break\n",
    "    \n",
    "    # Calculate approximation ratio\n",
    "    if worst_obj != best_obj:\n",
    "        ar = (worst_obj - current_obj) / (worst_obj - best_obj)\n",
    "    else:\n",
    "        ar = 1.0\n",
    "    \n",
    "    return max(0.0, min(1.0, ar))  # Clamp to [0, 1]\n",
    "\n",
    "\n",
    "def main(pickle_file: str, partition_size: int = 15, q: float = 0.5, \n",
    "         K_total: int = 40, p: int = 5, scale: float = 1.0, \n",
    "         n_obs: Optional[int] = None, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Main function using advanced partitioning with QAOA solving.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pickle_file : str\n",
    "        Path to the pickle file containing the portfolio graph\n",
    "    partition_size : int\n",
    "        Target size of each partition (default: 15)\n",
    "    q : float\n",
    "        Risk aversion parameter\n",
    "    K_total : int\n",
    "        Total number of assets to select across all partitions\n",
    "    p : int\n",
    "        QAOA circuit depth\n",
    "    scale : float\n",
    "        Scaling factor for the problem\n",
    "    n_obs : int\n",
    "        Number of observations for RMT denoising\n",
    "    verbose : bool\n",
    "        Whether to print progress information\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Load the graph and extract portfolio data\n",
    "    if verbose:\n",
    "        print(\"Loading portfolio data from pickle file...\")\n",
    "    sigma, mu = load_portfolio_data(pickle_file)\n",
    "    N = len(mu)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Loaded {N} assets\")\n",
    "        print(f\"Expected returns range: [{mu.min():.4f}, {mu.max():.4f}]\")\n",
    "        print(f\"Covariance range: [{sigma.min():.4f}, {sigma.max():.4f}]\")\n",
    "    \n",
    "    # Step 2: Advanced partitioning with RMT\n",
    "    if verbose:\n",
    "        print(f\"\\nApplying RMT denoising and partitioning into {partition_size}-asset subproblems...\")\n",
    "    \n",
    "    partitioned_mus, partitioned_sigmas, mappings = partition_with_rmt(\n",
    "        sigma, mu, partition_size, n_obs\n",
    "    )\n",
    "    num_partitions = len(mappings)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Created {num_partitions} partitions\")\n",
    "        print(f\"Partition sizes: {[len(m) for m in mappings]}\")\n",
    "    \n",
    "    # Step 3: Distribute K across partitions\n",
    "    K_per_partition = []\n",
    "    remaining_K = K_total\n",
    "    \n",
    "    for i, mapping in enumerate(mappings):\n",
    "        partition_size_actual = len(mapping)\n",
    "        if i < num_partitions - 1:\n",
    "            k_i = int(K_total * partition_size_actual / N)\n",
    "            k_i = min(k_i, partition_size_actual)\n",
    "            K_per_partition.append(k_i)\n",
    "            remaining_K -= k_i\n",
    "        else:\n",
    "            K_per_partition.append(min(remaining_K, partition_size_actual))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"K distribution across partitions: {K_per_partition}\")\n",
    "    \n",
    "    # Step 4: Solve each subproblem with QAOA\n",
    "    if verbose:\n",
    "        print(f\"\\nSolving {num_partitions} subproblems with QAOA (p={p})...\")\n",
    "    \n",
    "    sub_solutions = []\n",
    "    sub_approximation_ratios = []\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    for i in range(num_partitions):\n",
    "        if verbose:\n",
    "            print(f\"\\nPartition {i+1}/{num_partitions}:\")\n",
    "        \n",
    "        # Create subproblem with risk rebalancing\n",
    "        sub_problem = get_sub_problem_with_rebalancing(\n",
    "            sigma=partitioned_sigmas[i],\n",
    "            mu=partitioned_mus[i],\n",
    "            K=K_per_partition[i],\n",
    "            q=q,\n",
    "            seed=1,\n",
    "            scale=scale,\n",
    "            full_sigma=sigma,\n",
    "            full_mu=mu\n",
    "        )\n",
    "        \n",
    "        # Solve with QAOA\n",
    "        bitstring, opt_params, sub_ar = solve_qaoa_subproblem(sub_problem, p=p, verbose=verbose)\n",
    "        sub_solutions.append(bitstring)\n",
    "        sub_approximation_ratios.append(sub_ar)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  Solution: {bitstring} (selected {bitstring.count('1')} assets)\")\n",
    "    \n",
    "    # Step 5: Map solutions back to full problem\n",
    "    if verbose:\n",
    "        print(\"\\nMapping subproblem solutions to full solution...\")\n",
    "    \n",
    "    full_solution = map_qaoa_outputs_to_full_vector(sub_solutions, mappings, N)\n",
    "    \n",
    "    total_time = time.time() - total_start_time\n",
    "    \n",
    "    # Step 6: Evaluate final solution\n",
    "    selected_assets = np.where(full_solution == 1)[0]\n",
    "    num_selected = len(selected_assets)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFinal solution:\")\n",
    "        print(f\"  Total assets selected: {num_selected} (target: {K_total})\")\n",
    "        print(f\"  Selected asset indices: {selected_assets.tolist()}\")\n",
    "        print(f\"  Total computation time: {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate portfolio metrics\n",
    "    x = full_solution\n",
    "    portfolio_return = np.dot(mu, x)\n",
    "    portfolio_risk = np.dot(x, np.dot(sigma, x))\n",
    "    objective_value = q * portfolio_risk - portfolio_return\n",
    "    \n",
    "    # Calculate final approximation ratio\n",
    "    if verbose:\n",
    "        print(\"\\nCalculating final approximation ratio...\")\n",
    "    \n",
    "    final_ar = calculate_final_approximation_ratio(x, sigma, mu, K_total, q)\n",
    "    \n",
    "    # Calculate average subproblem AR\n",
    "    avg_sub_ar = np.mean(sub_approximation_ratios) if sub_approximation_ratios else 0\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nPortfolio metrics:\")\n",
    "        print(f\"  Expected return: {portfolio_return:.6f}\")\n",
    "        print(f\"  Risk (variance): {portfolio_risk:.6f}\")\n",
    "        print(f\"  Objective value: {objective_value:.6f}\")\n",
    "        print(f\"\\nApproximation Ratios:\")\n",
    "        print(f\"  Average subproblem AR: {avg_sub_ar:.6f}\")\n",
    "        print(f\"  Final solution AR: {final_ar:.6f}\")\n",
    "        print(f\"  {'='*50}\")\n",
    "        print(f\"  FINAL AR: {final_ar:.3f}\")\n",
    "        print(f\"  {'='*50}\")\n",
    "    \n",
    "    return full_solution, selected_assets, {\n",
    "        'return': portfolio_return,\n",
    "        'risk': portfolio_risk,\n",
    "        'objective': objective_value,\n",
    "        'approximation_ratio': final_ar,\n",
    "        'subproblem_approximation_ratios': sub_approximation_ratios,\n",
    "        'average_subproblem_ar': avg_sub_ar,\n",
    "        'computation_time': total_time,\n",
    "        'num_partitions': num_partitions\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    PICKLE_FILE = \"subgraph_150.pickle\"\n",
    "    PARTITION_SIZE = 7  # Each partition will have ~15 assets\n",
    "    Q = 0.5  # Risk aversion parameter\n",
    "    K_TOTAL = 30  # Total number of assets to select\n",
    "    P = 5  # QAOA depth\n",
    "    SCALE = 1.0  # Scaling factor\n",
    "    N_OBS = None  # Number of observations (5 years of daily data)\n",
    "    \n",
    "    # Run the advanced partitioned portfolio optimization\n",
    "    solution, selected_assets, metrics = main(\n",
    "        pickle_file=PICKLE_FILE,\n",
    "        partition_size=PARTITION_SIZE,\n",
    "        q=Q,\n",
    "        K_total=K_TOTAL,\n",
    "        p=P,\n",
    "        scale=SCALE,\n",
    "        n_obs=N_OBS,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'solution': solution,\n",
    "        'selected_assets': selected_assets,\n",
    "        'metrics': metrics,\n",
    "        'parameters': {\n",
    "            'partition_size': PARTITION_SIZE,\n",
    "            'q': Q,\n",
    "            'K_total': K_TOTAL,\n",
    "            'p': P,\n",
    "            'scale': SCALE,\n",
    "            'n_obs': N_OBS\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # with open('advanced_partitioned_qaoa_results.pickle', 'wb') as f:\n",
    "    #     pickle.dump(results, f)\n",
    "    \n",
    "    # print(\"\\nResults saved to 'advanced_partitioned_qaoa_results.pickle'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
